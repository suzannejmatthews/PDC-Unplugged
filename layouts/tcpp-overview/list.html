{{ partial "header" . }}



<main align="left">

<h1>TCPP View</h1>

 <p>This view classifies PDC unplugged activities by their classification according to the <a href="" target=_blank>NSF/IEEE-TCPP Curriculum Initiative on Parallel 
   and Distributed Computing</a> (TCPP 2012) which aims to articulate the core PDC areas that an undergraduate computer science student should cover in the course 
   of their undergraduate education. The TCPP report informed the creation of the PD topic area in CS2013, and lists several cross-cutting topics. For the immediate, our 
   focus is on identifying unplugged activities that satisfy each of topics suggested for Core courses (CS1, CS2, Systems and DS/A); Advanced/Elective courses are not 
   covered. Bloom's Taxonomy is used by TCPP to help solidify the level of coverage of each topic:</p>
   <p><ul>
    <li><b>Know</b>: Know the term (basic literacy)</li>
    <li><b>Comprehend:</b> Comprehend so as to paraphrase/illustrate</li>
    <li><b>Apply:</b> Apply it in some way (requires operational command)</li>
  </ul>
  </p>
  <p>Each topic is preceded by one of these terms. Use the Bloom taxonomy classification to gauge the level of topic coverage in a course.</p>

  <h3>Architecture Topics (<a href="{{ .Site.BaseURL }}tcpp/tcpp_architecture">{{ len .Site.Taxonomies.tcpp.tcpp_architecture }} Activities  </a>)</h3>

<table >
<tr>    <th width="40%">Topic</th>  <th>Suggested Course</th>  <th>Unplugged Activities</th>    </tr>
<tr><td> <u>Comprehend Parallel Taxonomy</u>: Flynn's taxonomy, data vs. control parallelism, shared/distributed memory (0.5 hours)</td><td>Systems</td>
<td>
   {{ range first 2 .Site.Taxonomies.tcppdetails.c_taxonomy }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_taxonomy">See All ({{ len .Site.Taxonomies.tcppdetails.c_taxonomy }}) </a> 
</td></tr>

<tr><td> <u>Know Superscalar (ILP)</u>: Describe opportunities for multiple instruction issue and execution (0.25-1 hours)</td><td>Systems</td><td></td></tr> 
<tr><td> <u>Know SIMD/Vector (e.g., SSE, Cray)</u>: Describe uses of SIMD/Vector (same operation on multiple data items), e.g., accelerating graphics for games. (0.1-0.5 hours)</td><td>Systems</td>
<td>
  {{ range first 2 .Site.Taxonomies.tcppdetails.k_simdvector }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_simdvector">See All ({{ len .Site.Taxonomies.tcppdetails.k_simdvector }}) </a>
</td></tr>
<tr><td><u>Know Pipelines (Single vs. multicycle)</u>: Describe basic pipelining process (multiple instructions can execute at the same time), describe stages of instruction execution (1-2 hours)</td><td>Systems</td><td></td></tr>
<tr><td><u>Know Streams (e.g., GPU)</u>: Know that stream-based architecture exists in GPUs for graphics (0.1-0.5 hours)</td><td>Systems</td><td></td></tr> 
<tr><td><u>Know MIMD</u>: Identify MIMD instances in practice (multicore, cluster, e.g.), and know the difference between execution of tasks and threads (0.1-0.5 hours)</td><td>Systems</td>
  <td>
     {{ range first 2 .Site.Taxonomies.tcppdetails.k_mimd }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_mimd">See All ({{ len .Site.Taxonomies.tcppdetails.k_mimd }}) </a>

  </td>
</tr>
<tr><td><u>Know Simultaneous MultiThreading</u>: Distinguish SMT from multicore (based on which resources are shared) (0.2-0.5 hours)</td><td>Systems</td><td></td></tr>
<tr><td><u>Comprehend Multicore</u>: Describe how cores share resources (cache, memory) and resolve conflicts (0.5-1 hours)</td><td>Systems</td>
<td>
    {{ range first 2 .Site.Taxonomies.tcppdetails.c_multicore }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_multicore">See All ({{ len .Site.Taxonomies.tcppdetails.c_multicore }}) </a>

</td>
</tr>
<tr><td> <u>Know Heterogeneous (e.g., Cell,on-chip GPU)</u>: Recognize that multicore may not all be the same kind of core (0.1-0.5 hours)</td><td>Systems</td>
  <td>
     {{ range first 2 .Site.Taxonomies.tcppdetails.k_heterogeneous }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_heterogeneous">See All ({{ len .Site.Taxonomies.tcppdetails.k_heterogeneous }}) </a>
  </td></tr>
<tr><td> <u>Comprehend Shared vs. distributed memory (SMP Buses)</u>: Systems Single resource, limited bandwidth and latency, snooping, scalability issues (0.5-1 hours)</td><td>Systems</td>
  <td>
         {{ range first 2 .Site.Taxonomies.tcppdetails.c_bus }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_bus">See All ({{ len .Site.Taxonomies.tcppdetails.c_bus }}) </a>

  </td></tr>
<tr><td> <u>Know Message Passing Latency</u>: Know the concept, implications for scaling, impact on work/communication ratio to achieve speedup (0.2-0.5 hours)</td><td>Systems</td>
  <td>
         {{ range first 2 .Site.Taxonomies.tcppdetails.k_latency }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_latency">See All ({{ len .Site.Taxonomies.tcppdetails.k_latency }}) </a>
  </td></tr>
<tr><td> <u> Know Message Passing Bandwidth</u>: Know the concept, how it limits sharing, and considerations of data movement cost (0.1-0.5 hours)</td><td>Systems</td>
  <td>
         {{ range first 2 .Site.Taxonomies.tcppdetails.k_bandwidth }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_bandwidth">See All ({{ len .Site.Taxonomies.tcppdetails.k_bandwidth }}) </a>
  </td></tr>
<tr><td> <u>Comprehend Cache organization</u> Know the cache hierarchies, shared caches (as opposed to private caches) result in coherency and performance issues for software (0.2 to 1 hours) </td><td>Systems</td><td></td></tr>
<tr><td> <u>Know Floating Point Range</u>: Understand that range is limited, implications of infinities</td><td>CS1/CS2/Systems</td><td></td></tr>
<tr><td> <u>Know Floating Point Precision</u>: How single and double precision floating point numbers impact software performance (0.1-0.5 hours)</td><td>CS1/CS2/Systems</td><td></td></tr>
<tr><td> <u>Know Floating Point Error Propagation</u>: Understand NaN, Infinity values and how they affect computations and exception handling (0.1-0.5 hours)</td><td>CS2</td><td></td></tr>
<tr><td> <u>Know Floating Point IEEE 754 standard</u>: Representation, range, precision, rounding, NaN, infinities, subnormals, comparison, effects of casting to other types (0.5-1 hours)</td><td>CS1/CS2/Systems</td><td></td></tr>
<tr><td> <u>Comprehend Cycles per Instruction (CPI)</u>: Number of clock cycles for instructions, understand the performance of processor implementation, various pipelined implementations (0.25-1 hours) </td><td>Systems</td><td></td></tr>
<tr><td> <u>Know Performance Benchmark - Spec Mark</u>: Awareness of pitfalls in relying on averages (different averages can alter perception of which architecture is faster) (0.25-0.5 hours)</td><td>Systems</td><td></td></tr>
<tr><td> <u>Comprehend Peak Performance</u>: Understanding peak performance, how it is rarely valid for estimating real performance, illustrate fallacies (0.1-0.5 hours)</td><td>Systems</td><td></td></tr>
<tr><td> <u>Know MIPS/FLOPS</u>: Understand meaning of terms (0.1 hours)</td><td>Systems</td><td></td></tr>
<tr><td> <u>Comprehend Peak Vs. Sustained Performance</u>: Know difference between peak and sustained performance, how to define, measure, different benchmarks (0.1-0.5 hours)</td><td>Systems</td><td></td></tr>
</table>


<p><br></p>


<h3>Programming Topics (<a href="{{ .Site.BaseURL }}tcpp/tcpp_programming">{{ len .Site.Taxonomies.tcpp.tcpp_programming }} Activities </a>) </h3>

<table>
<tr>    <th width="40%">Topic</th>  <th>Suggested Course</th>  <th>Unplugged Activities</th>    </tr>
<tr><td> <u>Know SIMD</u>: Understand common vector operations including element-by-element operations and reductions (0.5 hours)</td><td>CS2/Systems</td><td></td></tr>
<tr><td> <u>Know SIMD  (Process vector extensions)</u> : Know examples - SSE/Altivec macros</td><td>Systems</td><td></td></tr>
<tr><td> <u> Apply Shared Memory</u>: Be able to write correct thread-based programs (protecting shared data) and understand how to obtain speed up (2 hours).</td><td>CS2/DSA/ProgLang</td><td></td></tr>
<tr><td> <u>Know Shared Memory Language Extensions</u>: Know about language extensions for parallel programming. Illustration from Cilk (spawn/join) and Java (Java threads)</td><td>CS2/DSA/ProgLang</td><td></td></tr>
<tr><td> <u>Comprehend Shared Memory Compiler Directives</u>: Understand what simple directives, such as those of OpenMP, mean (parallel for, concurrent section), pragmas show examples</td><td>CS2/DSA/ProgLang</td><td></td></tr>
<tr><td> <u> Comprehend Shared Memory Libraries</u>: Know one in detail, and know of the existence of some other example libraries such as Pthreads, Pfunc, Intel's TBB (Thread building blocks), Microsoft's TPL (Task Parallel Library), etc.</td><td>CS2/DSA/ProgLang</td><td></td></tr>
 <tr><td> <u>Comprehend Distributed Memory</u>: Know basic notions of messaging among processes, different ways of message passing, collective operations (1 hour)</td><td>DSA/Systems</td>
  <td>
    {{ range first 2 .Site.Taxonomies.tcppdetails.c_distributedmemory }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_distributedmemory">See All ({{ len .Site.Taxonomies.tcppdetails.c_distributedmemory }}) </a>
  </td></tr> 
 <tr><td> <u>Comprehend Client Server</u>: Know notions of invoking and providing services (e.g., RPC, RMI, web services) - understand these as concurrent processes (1 hour)</td><td>DSA/Systems</td><td></td></tr> 
 <tr><td> <u>Know Hybrid</u>: Know the notion of programming over multiple classes of machines simultaneously (CPU, GPU, etc.) (0.5 hours)</td><td>Systems</td>
  <td>
          {{ range first 2 .Site.Taxonomies.tcppdetails.k_hybrid }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_hybrid">See All ({{ len .Site.Taxonomies.tcppdetails.k_hybrid }}) </a>
  </td></tr> 
 <tr><td> <u>Apply Task/thread spawning</u>: Be able to write correct programs with threads, synchronize (fork-join, producer/consumer, etc.), use dynamic threads (in number and possibly
recursively) thread creation (e.g. Pthreads, Java threads, etc.);  builds on shared memory topic above (1 hour)</td><td>CS2/DSA</td><td></td></tr> 
<tr><td> <u>Comprehend SPMD</u>: Understand how SPMD program is written and how it executes (1 hour)</td><td>CS2/DSA</td>
  <td>
   {{ range first 2 .Site.Taxonomies.tcppdetails.c_spmd }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_spmd">See All ({{ len .Site.Taxonomies.tcppdetails.c_spmd }}) </a>
  </td></tr> 
<tr><td> <u>Comprehend SPMD Notations</u>: Know the existence of highly threaded data parallel notations (e.g., CUDA, OpenCL), message passing (e.g, MPI), and some others (e.g., Global Arrays, BSP library)</td><td>CS2/DSA</td><td></td></tr> 
<tr><td> <u>Apply Data parallel</u>: Be able to write a correct data parallel program for shared-memory machines and get speedup, should do an exercise. Understand relation between different notations for data parallel: Array notations, SPMD, and parallel loops. Builds on shared memory topic above. (1 hour)</td><td>CS2/DSA/ProgLang</td>
  <td>
      {{ range first 2 .Site.Taxonomies.tcppdetails.a_dataparallel }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/a_dataparallel">See All ({{ len .Site.Taxonomies.tcppdetails.a_dataparallel }}) </a>
  </td></tr>
<tr><td> <u>Apply Parallel loops for shared memory</u>: Know, through an example, one way to implement parallel loops, understand collision/dependencies across iterations (e.g., OpenMP, Intel's TBB) for shared memory</td><td>CS2/DSA/ProgLang</td><td></td></tr>
<tr><td> <u>Know Tasks and threads</u>: Understand what it means to create and assign work to threads/processes in a parallel program, and know of at least one way do that (e.g., OpenMP, Intel TBB, etc.) (0.5 hours)</td><td>CS2/DSA/Systems/ProgLang</td><td>
      {{ range first 2 .Site.Taxonomies.tcppdetails.k_tasksthreads }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_tasksthreads">See All ({{ len .Site.Taxonomies.tcppdetails.k_tasksthreads }}) </a>
</td></tr>
<tr><td> <u>Apply Critical regions</u>: Be able to write shared memory programs that use critical regions for synchronization</td><td>CS2/DSA/Systems</td>
  <td>
      {{ range first 2 .Site.Taxonomies.tcppdetails.a_criticalregions }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/a_criticalregions">See All ({{ len .Site.Taxonomies.tcppdetails.a_criticalregions }}) </a>
  </td></tr>
<tr><td> <u>Apply Producer-consumer</u>: Be able to write shared memory programs that use the producer-consumer pattern to share data and synchronize threads</td><td>CS2/DSA/Systems</td><td></td></tr>
<tr><td> <u>Know Monitors</u>: Understand how to use monitors for synchronization</td><td>CS2/DSA/Systems</td><td></td></tr>
<tr><td> <u>Comprehend Deadlocks</u>: Understand what a deadlock is, and methods for detecting and preventing them</td><td>DSA/Systems</td>
  <td>
      {{ range first 2 .Site.Taxonomies.tcppdetails.c_deadlocks }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_deadlocks">See All ({{ len .Site.Taxonomies.tcppdetails.c_deadlocks }}) </a>
  </td></tr>
<tr><td> <u>Know Data Races</u>:  Know what a data race is, and how to use synchronization to prevent it</td><td>DSA/Systems</td>
  <td>
          {{ range first 2 .Site.Taxonomies.tcppdetails.k_dataraces }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_dataraces">See All ({{ len .Site.Taxonomies.tcppdetails.k_dataraces }}) </a>
  </td></tr>
<tr><td> <u>Know Tools to detect concurrency defects</u>: Know the existence of tools to detect race conditions (e.g., Eraser) (0.5 hours)</td><td>DSA/Systems</td><td></td></tr>
<tr><td> <u>Comprehend Computation Decomposition Strategies</u>: Understand different ways to assign computations to threads or processes</td><td>CS2/DSA</td>
<td>
    {{ range first 2 .Site.Taxonomies.tcppdetails.c_decomposition }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_decomposition">See All ({{ len .Site.Taxonomies.tcppdetails.c_decomposition }}) </a>
</td></tr>
<tr><td> <u>Comprehend Owner Computes Rule</u>: Understand how to assign loop iterations to threads based on which thread/process owns the data element(s) written in an iteration</td><td>CS2/DSA</td><td></td></tr>
<tr><td> <u>Comprehend Decomposition into Atomic Tasks</u>: Understand how to decompose computations into tasks with communication only at the beginning and end of each task, and assign them to threads/processes</td><td>CS2/DSA</td><td></td></tr>
<tr><td> <u>Comprehend Load balancing</u>: Understand the effects of load imbalances on performance, and ways to balance load across threads or processes (1 hour)</td><td>DSA/Systems</td>
  <td>
    {{ range first 2 .Site.Taxonomies.tcppdetails.c_loadbalancing }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_loadbalancing">See All ({{ len .Site.Taxonomies.tcppdetails.c_loadbalancing }}) </a>
  </td></tr>
<tr><td> <u> Comprehend Static Scheduling and mapping</u>: Understand how to map and schedule computations before runtime</td><td>DSA/Systems</td><td>
        {{ range first 2 .Site.Taxonomies.tcppdetails.c_static }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_static">See All ({{ len .Site.Taxonomies.tcppdetails.c_static }}) </a>
</td></tr>
<tr><td> <u> Comprehend Dynamic Scheduling and mapping</u>: Understand how to map and schedule computations at runtime</td><td>DSA/Systems</td>
  <td>
          {{ range first 2 .Site.Taxonomies.tcppdetails.c_dynamic }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_dynamic">See All ({{ len .Site.Taxonomies.tcppdetails.c_dynamic }}) </a>
  </td></tr>
<tr><td> <u> Know Data</u>: Understand impact of data distribution, layout and locality on performance; know false sharing and its impact on performance (e.g., in a cyclic mapping in a parallel loop); notion that transfer of data has fixed cost plus bit rate (irrespective of transfer from memory or inter-processor) (1 hour)</td><td>DSA/ProgLang</td>
  <td>
          {{ range first 2 .Site.Taxonomies.tcppdetails.k_data }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_data">See All ({{ len .Site.Taxonomies.tcppdetails.k_data }}) </a>
  </td></tr>
<tr><td> <u> Know Data Distribution</u>: Know what block, cyclic, and block-cyclic data distributions are, and what it means to distribute data across multiple threads/processes</td><td>DSA/ProgLang</td><td></td></tr>
<tr><td> <u> Know Data layout</u>: Know how to lay out data in memory to get improve performance (memory hierarchy)</td><td>DSA/ProgLang</td><td></td></tr>
<tr><td> <u> Know Data locality</u>: Know what spatial and temporal locality are, and how to organize data to take advantage of them</td><td>DSA/ProgLang</td><td></td></tr>
<tr><td> <u> Know False sharing</u>: Know that for cache coherent shared memory systems, data is kept coherent in blocks, not individual words, and how to avoid false sharing across threads of data for a block</td><td>DSA/ProgLang</td><td></td></tr>
<tr><td> <u> Know Performance Tools</u>: Know of tools for runtime monitoring (e.g., gprof, monitoring tools Vtune) (0.5 hours)</td><td>DSA/Systems</td><td></td></tr>
<tr><td> <u> Comprehend Speedup</u>: Understand how to compute speedup, and what it means</td><td>CS2/DSA</td>
  <td>
          {{ range first 2 .Site.Taxonomies.tcppdetails.c_speedup }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/c_speedup">See All ({{ len .Site.Taxonomies.tcppdetails.c_speedup }}) </a>
  </td></tr>
<tr><td> <u> Comprehend Efficiency</u>: Understand how to compute efficiency, and why it matters</td><td>CS2/DSA</td><td></td></tr>
<tr><td> <u> Know Amdahl's Law</u>: Know that speedup is limited by the sequential portion of a parallel program, if problem size is kept fixed</td><td>CS2/DSA</td>
  <td>
          {{ range first 2 .Site.Taxonomies.tcppdetails.k_amdahl }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}tcppdetails/k_amdahl">See All ({{ len .Site.Taxonomies.tcppdetails.k_amdahl }}) </a>
  </td></tr>
<tr><td> <u> Know Gustafson’s Law</u>: Understand the idea of weak scaling, where problem size increases as the number of processes/threads increases</td><td>CS2/DSA</td><td></td></tr>
</table>


<p><br></p>






<p><br></p>
<h3>Parallel Algorithms, Analysis, &amp; Programming ( <a href="{{ .Site.BaseURL }}cs2013/pd_parallelalgorithms">{{ len .Site.Taxonomies.cs2013.pd_parallelalgorithms }} Activities  </a>) </h3>
<table>
    <tr>
    <th width = "60%">Learning Outcome</th> 
    <th>Unplugged Activities</th>
    <tr><th>Core Tier 2 </th></tr>
<tr><td> 1. Define "critical path", "work", and "span". [Familiarity] </td><td></td></tr>
<tr><td> 2. Compute the work and span, and determine the critical path with respect to a parallel execution diagram. [Usage] </td><td></td></tr>
<tr><td> 3. Define "speed-up" and explain the notion of an algorithm's scalability in this regard. [Familiarity] </td>
  <td>
      {{ range first 2 .Site.Taxonomies.cs2013details.algo_3 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/algo_3">See All ({{ len .Site.Taxonomies.cs2013details.algo_3 }}) </a>
  </td>
</tr>
<tr><td> 4. Identify independent tasks in a program that may be parallelized. [Usage] </td>
  <td>
       {{ range first 2 .Site.Taxonomies.cs2013details.algo_4 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/algo_4">See All ({{ len .Site.Taxonomies.cs2013details.algo_4 }}) </a>
  </td></tr>
<tr><td> 5. Characterize features of a workload that allow or prevent it from being naturally parallelized. [Familiarity]</td><td>
 {{ range first 2 .Site.Taxonomies.cs2013details.algo_5 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/algo_5">See All ({{ len .Site.Taxonomies.cs2013details.algo_5 }}) </a>

</td></tr>
<tr><td> 6. Implement a parallel divide-and-conquer (and/or graph algorithm) and empirically measure its performance relative to its sequential analog. </td>
<td>
 {{ range first 2 .Site.Taxonomies.cs2013details.algo_6 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>;
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/algo_6">See All ({{ len .Site.Taxonomies.cs2013details.algo_6 }}) </a>
</td></tr>
<tr><td> 7. Decompose a problem (e.g., counting the number of occurrences of some word in a document) via map and reduce operations.</td><td></td></tr>
<tr><th>Elective </th></tr>
<tr><td> 8. Provide an example of a problem that fits the producer-consumer paradigm. [Familiarity]</td><td></td></tr>
<tr><td> 9. Give examples of problems where pipelining would be an effective means of parallelization. [Familiarity]</td>
<td>
        {{ range first 2 .Site.Taxonomies.cs2013details.algo_9 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/algo_9">See All ({{ len .Site.Taxonomies.cs2013details.algo_9 }}) </a>
</td></tr>


<tr><td> 10. Implement a parallel matrix algorithm. [Usage]</td>
  <td>
        {{ range first 2 .Site.Taxonomies.cs2013details.algo_10 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/algo_10">See All ({{ len .Site.Taxonomies.cs2013details.algo_10 }}) </a>
  </td></tr>
<tr><td> 11. Identify issues that arise in producer-consumer algorithms and mechanisms that may be used for addressing them. [Familiarity]</td><td></td></tr>
</table>

<p><br></p>

<h3>Parallel Architecture (<a href="{{ .Site.BaseURL }}cs2013/pd_parallelarchitecture">{{ len .Site.Taxonomies.cs2013.pd_parallelarchitecture }} Activities  </a>) </h3>

<table>
    <tr>
    <th width="60%">Learning Outcome</th> 
    <th>Unplugged Activities</th>
    <tr><th>Core Tier 1 </th></tr>
<tr><td> 1. Explain the differences between shared and distributed memory. [Familiarity] [Core-Tier2] </td><td>
   {{ range first 2 .Site.Taxonomies.cs2013details.arch_1 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/arch_1">See All ({{ len .Site.Taxonomies.cs2013details.arch_1 }}) </a>
</td></tr>
 <tr><th>Core Tier 2 </th></tr>
<tr><td> 2. Describe the SMP architecture and note its key features. [Familiarity] </td>
  <td>
       {{ range first 2 .Site.Taxonomies.cs2013details.arch_2 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/arch_2">See All ({{ len .Site.Taxonomies.cs2013details.arch_2 }}) </a>
  </td></tr>
<tr><td> 3. Characterize the kinds of tasks that are a natural match for SIMD machines. [Familiarity]</td>
  <td>   {{ range first 2 .Site.Taxonomies.cs2013details.arch_3 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/arch_3">See All ({{ len .Site.Taxonomies.cs2013details.arch_3 }}) </a></td></tr>
 <tr><th>Elective </th></tr>
<tr><td>4. Describe the advantages and limitations of GPUs vs. CPUs. [Familiarity]</td>
  <td>   {{ range first 2 .Site.Taxonomies.cs2013details.arch_4 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/arch_4">See All ({{ len .Site.Taxonomies.cs2013details.arch_4 }}) </a></td></tr>
<tr><td>5. Explain the features of each classification in Flynn's taxonomy. [Familiarity]</td><td>
     {{ range first 2 .Site.Taxonomies.cs2013details.arch_5 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/arch_5">See All ({{ len .Site.Taxonomies.cs2013details.arch_5 }}) </a>
</td></tr>
<tr><td>6. Describe assembly-level support for atomic operations. [Familiarity]</td>
  <td>   </td></tr>
<tr><td>7. Describe the challenges in maintaining cache coherence. [Familiarity]</td><td>

</td></tr>
<tr><td>8. Describe the key performance challenges in different memory and distributed system topologies. [Familiarity]</td><td>
    {{ range first 2 .Site.Taxonomies.cs2013details.arch_8 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/arch_8">See All ({{ len .Site.Taxonomies.cs2013details.arch_8 }}) </a>
</td></tr>
</table>

<p><br></p>


<h3>Parallel Performance (<a href="{{ .Site.BaseURL }}cs2013/pd_parallelperformance">{{ len .Site.Taxonomies.cs2013.pd_parallelperformance }} Activities  </a>
)</h3>

<table>
    <tr>
    <th width="60%">Learning Outcome</th> 
    <th>Unplugged Activities</th>
    <tr><th>Elective </th></tr>
<tr><td>1. Detect and correct a load imbalance. [Usage]</td><td>
{{ range first 2 .Site.Taxonomies.cs2013details.perf_1 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/perf_1">See All ({{ len .Site.Taxonomies.cs2013details.perf_1 }}) </a>
</td></tr>
<tr><td>2. Calculate the implications of Amdahl's law for a particular parallel algorithm (cross-reference SF/Evaluation for Amdahl's Law). [Usage]</td>
  <td>
    {{ range first 2 .Site.Taxonomies.cs2013details.perf_2 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/perf_2">See All ({{ len .Site.Taxonomies.cs2013details.perf_2 }}) </a></td></tr>
<tr><td>3. Describe how data distribution/layout can affect an algorithm's communication costs. [Familiarity]</td>
  <td>{{ range first 2 .Site.Taxonomies.cs2013details.perf_3 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/perf_3">See All ({{ len .Site.Taxonomies.cs2013details.perf_3 }}) </a></td></tr>
<tr><td>4. Detect and correct an instance of false sharing. [Usage]</td>
  <td></td></tr>
<tr><td>5. Explain the impact of scheduling on parallel performance. [Familiarity]</td><td>
  {{ range first 2 .Site.Taxonomies.cs2013details.perf_5 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/perf_5">See All ({{ len .Site.Taxonomies.cs2013details.perf_5 }}) </a>
</td></tr>
<tr><td>6. Explain performance impacts of data locality. [Familiarity]</td><td>

</td></tr>
<tr><td>7. Explain the impact and trade-off related to power usage on parallel performance. [Familiarity]</td><td>
</td></tr>
</table>

<p><br></p>

<h3>Cloud Computing (<a href="{{ .Site.BaseURL }}cs2013/pd_cloudcomputing">{{ len .Site.Taxonomies.cs2013.pd_cloudcomputing }} Activities  </a>)</h3>

<table>
    <tr>
    <th width="60%">Learning Outcome</th> 
    <th>Unplugged Activities</th>
    <tr><th>Elective </th></tr>
<tr><td>1. Discuss the importance of elasticity and resource management in cloud computing. [Familiarity]</td><td></td></tr>
<tr><td>2. Explain strategies to synchronize a common view of shared data across a collection of devices.[Familiarity]</td><td>
   {{ range first 2 .Site.Taxonomies.cs2013details.cc_2 }}
    <a href="{{ .Permalink }}">{{ .LinkTitle }}</a>; 
    {{ end }}
    <a href="{{ $.Site.BaseURL }}cs2013details/cc_2">See All ({{ len .Site.Taxonomies.cs2013details.cc_2 }}) </a>
</td></tr>
<tr><td>3. Explain the advantages and disadvantages of using virtualized infrastructure. [Familiarity]</td><td></td></tr>
<tr><td>4. Deploy an application that uses cloud infrastructure for computing and/or data resources. [Usage]</td><td></td></tr>
<tr><td>5. Appropriately partition an application between a client and resources. [Usage]</td><td></td></tr>
</table>
</main>

{{ partial "footer" . }}
